{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Exercise 1.2- Group 2\n",
    "**s232161-Xiaoyu Yan 25%**\\\n",
    "**s Marcel Zelent 25%**\\\n",
    "**s Linna Li 25%**\\\n",
    "**s Nicolaus 25%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation import ActivationFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\" \n",
    "      Perceptron neuron model\n",
    "      Parameters\n",
    "      ----------\n",
    "      n_inputs : int\n",
    "         Number of inputs\n",
    "      act_f : Subclass of `ActivationFunction`\n",
    "         Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, act_f):\n",
    "        \"\"\"\n",
    "         Perceptron class initialization\n",
    "         TODO: Write the code to initialize weights and save the given activation function\n",
    "        \"\"\"\n",
    "        if not isinstance(act_f, type) or not issubclass(act_f, ActivationFunction):\n",
    "            raise TypeError('act_f has to be a subclass of ActivationFunction (not a class instance).')\n",
    "        # weights\n",
    "        np.random.seed(42)\n",
    "        self.w = np.random.normal(0, 1, (n_inputs + 1,))#np.random.normal(mean, standard deviation, size)\n",
    "        # activation function\n",
    "        self.f = act_f()\n",
    "        self.bias = 1\n",
    "\n",
    "        if self.f is not None and not isinstance(self.f, ActivationFunction):\n",
    "            raise TypeError(\"self.f should be a class instance.\")\n",
    "\n",
    "    def activation(self, x):\n",
    "        \"\"\"\n",
    "         It computes the activation `a` given an input `x`\n",
    "         TODO: Fill in the function to provide the correct output\n",
    "         NB: Remember the bias\n",
    "        \"\"\"\n",
    "        a = np.dot(self.w[1:].T, x) + self.bias*self.w[0]\n",
    "        return a\n",
    "\n",
    "    def output(self, a):\n",
    "        \"\"\"\n",
    "         It computes the neuron output `y`, given the activation `a`\n",
    "         TODO: Fill in the function to provide the correct output\n",
    "        \"\"\"\n",
    "        y = self.f.forward(a)\n",
    "        return y\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "         It computes the neuron output `y`, given the input `x`\n",
    "         TODO: Fill in the function to provide the correct output\n",
    "        \"\"\"\n",
    "        a = self.activation(x)\n",
    "        y_out = self.output(a)\n",
    "        return y_out\n",
    "\n",
    "    def gradient(self, a):\n",
    "        \"\"\"\n",
    "         It computes the gradient of the activation function, given the activation `a`\n",
    "        \"\"\"\n",
    "        return self.f.gradient(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "    \"\"\" \n",
    "      Sigmoid activation: `f(x) = 1/(1+e^(-x))`\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "         Activation function output.\n",
    "         TODO: Change the function to return the correct value, given input `x`.\n",
    "        \"\"\"\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        \"\"\"\n",
    "         Activation function derivative.\n",
    "         TODO: Change the function to return the correct value, given input `x`.\n",
    "        \"\"\"\n",
    "        return self.forward(x) * (1 - self.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivation(ActivationFunction):\n",
    "    \"\"\" \n",
    "      Linear activation: `f(x) = x`\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "         Activation function output.\n",
    "         TODO: Change the function to return the correct value, given input `x`.\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "    def gradient(self, x):\n",
    "        \"\"\"\n",
    "         Activation function derivative.\n",
    "         TODO: Change the function to return the correct value, given input `x`.\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_inputs, num_units, act_f):\n",
    "        \"\"\" \n",
    "         Initialize the layer, creating `num_units` perceptrons with `num_inputs` each. \n",
    "        \"\"\"\n",
    "        # TODO Create the perceptrons required for the layer\n",
    "        self.num_units = num_units\n",
    "        self.ps = []\n",
    "        for i in range(num_units):\n",
    "            self.ps.append(Perceptron(num_inputs, act_f))\n",
    "\n",
    "    def activation(self, x):\n",
    "        \"\"\" Returns the activation `a` of all perceptrons in the layer, given the input vector`x`. \"\"\"\n",
    "        return np.array([p.activation(x) for p in self.ps])\n",
    "\n",
    "    def output(self, a):\n",
    "        \"\"\" Returns the output `o` of all perceptrons in the layer, given the activation vector `a`. \"\"\"\n",
    "        return np.array([p.output(ai) for p, ai in zip(self.ps, a)])\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\" Returns the output `o` of all perceptrons in the layer, given the input vector `x`. \"\"\"\n",
    "        return np.array([p.predict(x) for p in self.ps])\n",
    "\n",
    "    def gradient(self, a):\n",
    "        \"\"\" Returns the gradient of the activation function for all perceptrons in the layer, given the activation vector `a`. \"\"\"\n",
    "        return np.array([p.gradient(ai) for p, ai in zip(self.ps, a)])\n",
    "\n",
    "    def update_weights(self, dw):\n",
    "       \"\"\" \n",
    "       Update the weights of all of the perceptrons in the layer, given the weight change of each.\n",
    "       Input size: (n_inputs+1, n_units)\n",
    "       \"\"\"\n",
    "       for i in range(self.num_units):\n",
    "          self.ps[i].w += dw[:,i]\n",
    "\n",
    "    @property\n",
    "    def w(self):\n",
    "        \"\"\"\n",
    "         Returns the weights of the neurons in the layer.\n",
    "         Size: (n_inputs+1, n_units)\n",
    "        \"\"\"\n",
    "        return np.array([p.w for p in self.ps]).T\n",
    "\n",
    "    def import_weights(self, w):\n",
    "        \"\"\" \n",
    "         Import the weights of all of the perceptrons in the layer.\n",
    "         Input size: (n_inputs+1, n_units)\n",
    "        \"\"\"\n",
    "        for i in range(self.num_units):\n",
    "           self.ps[i].w = w[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67040836 0.67040836 0.67040836 0.67040836 0.67040836]\n",
      "[[ 0.49671415  0.49671415  0.49671415  0.49671415  0.49671415]\n",
      " [-0.1382643  -0.1382643  -0.1382643  -0.1382643  -0.1382643 ]\n",
      " [ 0.64768854  0.64768854  0.64768854  0.64768854  0.64768854]]\n"
     ]
    }
   ],
   "source": [
    "layer = Layer(2, 5, Sigmoid)\n",
    "pre = layer.predict([np.pi, 1])\n",
    "print(pre)\n",
    "print(layer.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2-3\n",
    "5 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\" \n",
    "      Multi-layer perceptron class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_inputs : int\n",
    "       Number of inputs\n",
    "    n_hidden_units : int\n",
    "       Number of units in the hidden layer\n",
    "    n_outputs : int\n",
    "       Number of outputs\n",
    "    alpha : float\n",
    "       Learning rate used for gradient descent\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, n_hidden_units, n_outputs, alpha=1e-3):\n",
    "       self.num_inputs = num_inputs\n",
    "       self.n_hidden_units = n_hidden_units\n",
    "       self.n_outputs = n_outputs\n",
    "\n",
    "       self.alpha = alpha\n",
    "\n",
    "       # TODO: Define a hidden layer and the output layer\n",
    "       self.l1 = Layer(self.num_inputs, self.n_hidden_units, Sigmoid)# hidden layer 1\n",
    "       self.l_out = Layer(self.n_hidden_units, self.n_outputs, LinearActivation) # output layer\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\" \n",
    "        Forward pass prediction given the input x\n",
    "        TODO: Write the function\n",
    "        \"\"\"\n",
    "        out1 = self.l1.predict(x)\n",
    "        out2 = self.l_out.predict(out1)\n",
    "        return out2\n",
    "\n",
    "    def train(self, inputs, outputs):\n",
    "        \"\"\"\n",
    "          Train the network \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        `x` : numpy array\n",
    "           Inputs (size: n_examples, n_inputs)\n",
    "        `t` : numpy array\n",
    "           Targets (size: n_examples, n_outputs)\n",
    "\n",
    "        TODO: Write the function to iterate through training examples and apply gradient descent to update the neuron weights\n",
    "        \"\"\"\n",
    "        # Loop over training examples\n",
    "        dw1 = np.zeros_like(self.l1.w)\n",
    "        dw_out = np.zeros_like(self.l_out.w)\n",
    "        \n",
    "        for x,t in zip(inputs, outputs):\n",
    "            # Forward pass\n",
    "\n",
    "            a1 = self.l1.activation(x)\n",
    "            o1 = self.l1.output(a1)\n",
    "            a_out = self.l_out.activation(o1)\n",
    "            o_out = self.l_out.output(a_out)\n",
    "\n",
    "            # Backpropagation\n",
    "            delta_out = (o_out - t) * self.l_out.gradient(a_out)\n",
    "            delta1 = self.l1.gradient(a1) * self.l_out.w[1:].dot(delta_out)\n",
    "\n",
    "            # Add weight change contributions to temporary array\n",
    "            o0 = np.insert(x, 0, 1)  # Add bias term\n",
    "            o1 = np.insert(o1, 0, 1)  # Add bias term\n",
    "\n",
    "            dw1 += delta1.reshape(-1, 1).dot(o0.reshape(1, -1)).T\n",
    "            dw_out += np.outer(o1, delta_out)\n",
    "\n",
    "               # Update weights\n",
    "        self.l1.update_weights(-self.alpha * dw1)\n",
    "        self.l_out.update_weights(-self.alpha * dw_out)\n",
    "\n",
    "        # Forward pass\n",
    "\n",
    "\n",
    "        # Backpropagation\n",
    "\n",
    "\n",
    "        # Add weight change contributions to temporary array\n",
    "         \n",
    "        # Update weights\n",
    "      \n",
    "        return None # remove this line\n",
    "\n",
    "    def export_weights(self):\n",
    "        return [self.l1.w, self.l2.w]\n",
    "   \n",
    "    def import_weights(self, ws):\n",
    "        if ws[0].shape == (self.l1.n_units, self.n_inputs+1) and ws[1].shape == (self.l2.n_units, self.l1.n_units+1):\n",
    "            print(\"Importing weights..\")\n",
    "            self.l1.import_weights(ws[0])\n",
    "            self.l2.import_weights(ws[1])\n",
    "        else:\n",
    "            print(\"Sizes do not match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49671415  0.49671415  0.49671415]\n",
      " [-0.1382643  -0.1382643  -0.1382643 ]\n",
      " [ 0.64768854  0.64768854  0.64768854]]\n",
      "[[ 0.49671415]\n",
      " [-0.1382643 ]\n",
      " [ 0.64768854]\n",
      " [ 1.52302986]]\n"
     ]
    }
   ],
   "source": [
    "xdata = [np.pi, 1]\n",
    "mlp = MLP(2, 3, 1)\n",
    "mlp.predict(xdata)\n",
    "print(mlp.l1.w)\n",
    "print(mlp.l_out.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prediction_error(model, x, t):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    x: xdata input\n",
    "    t: ground truth\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    for i in range(x.shape[0]):\n",
    "        y.append(model.predict(x[i, :]))\n",
    "    y = np.array(y)\n",
    "    n = t.shape[0]\n",
    "    error = np.sum((y - t)**2) / n\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.539173462972656\n"
     ]
    }
   ],
   "source": [
    "data = np.array( [ [0.5, 0.5, 0], [1.0, 0, 0], [2.0, 3.0, 0], [0, 1.0, 1], [0, 2.0, 1], [1.0, 2.2, 1] ] )\n",
    "xdata = data[:,:2]\n",
    "ydata = data[:,2]\n",
    "error = calc_prediction_error(mlp, xdata, ydata)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.583494870453666\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 0]])\n",
    "xdata = data[:, :2]\n",
    "ydata = data[:, 2]\n",
    "error = calc_prediction_error(mlp, xdata, ydata)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48525974]\n",
      "[0.51964597]\n",
      "[0.47703855]\n",
      "[0.51288852]\n"
     ]
    }
   ],
   "source": [
    "xor = MLP(2, 2, 1)\n",
    "error = []\n",
    "for epoch in range(1000):\n",
    "    xor.train(xdata, ydata)\n",
    "    error.append(calc_prediction_error(xor, xdata, ydata))\n",
    "for i in range(4):\n",
    "    print(xor.predict(xdata[i, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def linear_activation(x):\n",
    "    return x\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    weights_hidden = np.random.rand(hidden_size, input_size)\n",
    "    biases_hidden = np.zeros((hidden_size, 1))\n",
    "\n",
    "    weights_output = np.random.rand(output_size, hidden_size)\n",
    "    biases_output = np.zeros((output_size, 1))\n",
    "\n",
    "    parameters = {\n",
    "        'weights_hidden': weights_hidden,\n",
    "        'biases_hidden': biases_hidden,\n",
    "        'weights_output': weights_output,\n",
    "        'biases_output': biases_output\n",
    "    }\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(inputs, parameters):\n",
    "    # Hidden layer\n",
    "    hidden_activation = sigmoid(np.dot(parameters['weights_hidden'], inputs) + parameters['biases_hidden'])\n",
    "\n",
    "    # Output layer\n",
    "    output_activation = linear_activation(np.dot(parameters['weights_output'], hidden_activation) + parameters['biases_output'])\n",
    "\n",
    "    cache = {\n",
    "        'hidden_activation': hidden_activation,\n",
    "        'output_activation': output_activation\n",
    "    }\n",
    "\n",
    "    return output_activation, cache\n",
    "\n",
    "def backward_propagation(inputs, outputs, cache, parameters, learning_rate):\n",
    "    m = inputs.shape[1]\n",
    "\n",
    "    # Output layer\n",
    "    output_error = outputs - cache['output_activation']\n",
    "    output_delta = output_error  # Linear activation function derivative is 1\n",
    "\n",
    "    # Hidden layer\n",
    "    hidden_error = np.dot(parameters['weights_output'].T, output_delta)\n",
    "    hidden_delta = cache['hidden_activation'] * (1 - cache['hidden_activation']) * hidden_error\n",
    "\n",
    "    # Update parameters\n",
    "    parameters['weights_output'] += learning_rate * np.dot(output_delta, cache['hidden_activation'].T) / m\n",
    "    parameters['biases_output'] += learning_rate * np.sum(output_delta, axis=1, keepdims=True) / m\n",
    "\n",
    "    parameters['weights_hidden'] += learning_rate * np.dot(hidden_delta, inputs.T) / m\n",
    "    parameters['biases_hidden'] += learning_rate * np.sum(hidden_delta, axis=1, keepdims=True) / m\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def train_neural_network(inputs, outputs, hidden_size, learning_rate, epochs):\n",
    "    input_size = inputs.shape[0]\n",
    "    output_size = outputs.shape[0]\n",
    "\n",
    "    parameters = initialize_parameters(input_size, hidden_size, output_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        output_activation, cache = forward_propagation(inputs, parameters)\n",
    "        parameters = backward_propagation(inputs, outputs, cache, parameters, learning_rate)\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            cost = np.mean(np.square(outputs - output_activation)) / 2\n",
    "            print(f'Epoch {epoch}, Cost: {cost}')\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'inputs' and 'outputs' are your input and output data, with each column representing a data point\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "outputs = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "hidden_size = 2\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "trained_parameters = train_neural_network(inputs, outputs, hidden_size, learning_rate, epochs)\n",
    "\n",
    "# Test the trained neural network\n",
    "output_activation, _ = forward_propagation(inputs, trained_parameters)\n",
    "print(f'Output: {output_activation}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
